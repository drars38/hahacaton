from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
import pandas as pd
import numpy as np
from model import RuBertSentimentClassifier
import io
import json
import os
from datetime import datetime
import uuid

app = FastAPI(title="Sentiment Analysis API")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs("data", exist_ok=True)
os.makedirs("shared_data", exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RuBERT –º–æ–¥–µ–ª–∏
classifier = RuBertSentimentClassifier()
MODEL_PATH = "data"
TRAINING_DATA_PATH = "data/training_data.csv"


def ensure_model_exists():
    """–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏–Ω–∞—á–µ –æ–±—É—á–∏—Ç—å –Ω–∞ CSV –¥–∞–Ω–Ω—ã—Ö"""
    try:
        classifier.load(MODEL_PATH)
        print("‚úÖ RuBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –æ–±—É—á–∞–µ–º –Ω–∞ CSV –¥–∞–Ω–Ω—ã—Ö...")
        from train_model import train_and_save_model
        success = train_and_save_model(TRAINING_DATA_PATH)
        if not success:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª data/training_data.csv —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return False

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–Ω–æ–≤–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        try:
            classifier.load(MODEL_PATH)
            print("‚úÖ RuBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")
            return True
        except Exception as load_error:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: {load_error}")
            return False


# –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
model_ready = ensure_model_exists()


@app.get("/")
async def root():
    status = "ready" if model_ready else "training_required"
    return {
        "message": "Sentiment Analysis API with RuBERT",
        "status": status,
        "model_ready": model_ready,
        "model_type": "RuBERT"
    }


@app.get("/health")
async def health_check():
    status = "healthy" if model_ready else "needs_training"
    return {
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "model_ready": model_ready,
        "model_type": "RuBERT"
    }


@app.post("/predict")
async def predict_sentiment(file: UploadFile = File(...)):
    if not model_ready:
        raise HTTPException(
            status_code=503,
            detail="Model is not ready. Please ensure training data exists and model is trained."
        )

    try:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        file_id = str(uuid.uuid4())
        output_filename = f"results_{file_id}.csv"
        output_path = f"shared_data/{output_filename}"

        # –ß—Ç–µ–Ω–∏–µ CSV
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–∫–∏ 'text'
        if 'text' not in df.columns:
            raise HTTPException(status_code=400, detail="CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'text'")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é RuBERT
        texts = df['text'].tolist()
        print(f"üîÆ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é RuBERT...")
        predictions = classifier.predict(texts)

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame —Ç–æ–ª—å–∫–æ —Å –¥–≤—É–º—è –∫–æ–ª–æ–Ω–∫–∞–º–∏
        result_df = pd.DataFrame({
            'text': texts,
            'label': predictions
        })

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_df.to_csv(output_path, index=False)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞
        stats = result_df['label'].value_counts().to_dict()
        stats_named = {
            'negative': stats.get(0, 0),
            'neutral': stats.get(1, 0),
            'positive': stats.get(2, 0)
        }

        return {
            "message": "Prediction completed with RuBERT",
            "statistics": stats_named,
            "results_file": output_filename,
            "file_id": file_id,
            "total_records": len(result_df),
            "model_type": "RuBERT"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/retrain")
async def retrain_model(file: UploadFile = File(...)):
    """–ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ß—Ç–µ–Ω–∏–µ CSV —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º 'label' –≤–º–µ—Å—Ç–æ 'sentiment'
        required_columns = ['text', 'label']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            raise HTTPException(
                status_code=400,
                detail=f"CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ {required_columns}. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {missing_columns}"
            )

        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        texts = df['text'].tolist()
        labels = df['label'].astype(int).tolist()

        classifier.train(texts, labels)
        classifier.save(MODEL_PATH)

        global model_ready
        model_ready = True

        return {
            "message": "Model retrained successfully",
            "records_used": len(df),
            "label_distribution": df['label'].value_counts().to_dict()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/model/status")
async def model_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"""
    training_data_exists = os.path.exists(TRAINING_DATA_PATH)
    model_exists = os.path.exists(MODEL_PATH)

    # –ü—Ä–æ–≤–µ—Ä–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É training data –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    training_data_info = {}
    if training_data_exists:
        try:
            df = pd.read_csv(TRAINING_DATA_PATH)
            training_data_info = {
                "columns": list(df.columns),
                "records_count": len(df),
                "has_text_column": 'text' in df.columns,
                "has_label_column": 'label' in df.columns
            }
        except Exception as e:
            training_data_info = {"error": str(e)}

    return {
        "model_ready": model_ready,
        "training_data_exists": training_data_exists,
        "model_file_exists": model_exists,
        "training_data_path": TRAINING_DATA_PATH,
        "model_path": MODEL_PATH,
        "training_data_info": training_data_info
    }


@app.get("/download/{filename}")
async def download_file(filename: str):
    file_path = f"shared_data/{filename}"
    if os.path.exists(file_path):
        return FileResponse(
            file_path,
            media_type='text/csv',
            filename=f"sentiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        )
    else:
        raise HTTPException(status_code=404, detail="File not found")


@app.post("/evaluate")
async def evaluate_model(
        predictions_file: UploadFile = File(...),
        ground_truth_file: UploadFile = File(...)
):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"""
    try:
        print(f"üîç –ù–∞—á–∞–ª–æ evaluate: {predictions_file.filename}, {ground_truth_file.filename}")

        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤
        predictions_content = await predictions_file.read()
        ground_truth_content = await ground_truth_file.read()

        predictions_text = predictions_content.decode('utf-8')
        ground_truth_text = ground_truth_content.decode('utf-8')

        # –ß–∏—Ç–∞–µ–º CSV
        pred_df = pd.read_csv(io.StringIO(predictions_text))
        true_df = pd.read_csv(io.StringIO(ground_truth_text))

        print(f"üìä –ö–æ–ª–æ–Ω–∫–∏ predictions: {list(pred_df.columns)}")
        print(f"üìä –ö–æ–ª–æ–Ω–∫–∏ ground_truth: {list(true_df.columns)}")
        print(f"üìè –†–∞–∑–º–µ—Ä predictions: {len(pred_df)}")
        print(f"üìè –†–∞–∑–º–µ—Ä ground_truth: {len(true_df)}")

        # –ì–ò–ë–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–û–õ–û–ù–û–ö

        # –î–ª—è predictions –∏—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å sentiment
        sentiment_col = None
        for col in ['sentiment', 'label', 'sentiment_label', 'target']:
            if col in pred_df.columns:
                sentiment_col = col
                break

        if sentiment_col is None:
            available_cols = list(pred_df.columns)
            raise HTTPException(
                status_code=400,
                detail=f"Predictions file must contain sentiment column. Available columns: {available_cols}"
            )

        # –î–ª—è ground truth –∏—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        truth_col = None
        for col in ['label', 'sentiment', 'target', 'true_label']:
            if col in true_df.columns:
                truth_col = col
                break

        if truth_col is None:
            available_cols = list(true_df.columns)
            raise HTTPException(
                status_code=400,
                detail=f"Ground truth file must contain label column. Available columns: {available_cols}"
            )

        print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É predictions: '{sentiment_col}'")
        print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É ground_truth: '{truth_col}'")

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        from sklearn.metrics import f1_score, classification_report, accuracy_score

        y_true = true_df[truth_col]
        y_pred = pred_df[sentiment_col]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if len(y_true) != len(y_pred):
            print(f"‚ö†Ô∏è –†–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞: y_true={len(y_true)}, y_pred={len(y_pred)}")
            # –ë–µ—Ä–µ–º –º–∏–Ω–∏–º—É–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            min_len = min(len(y_true), len(y_pred))
            y_true = y_true[:min_len]
            y_pred = y_pred[:min_len]

        print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è y_true: {sorted(y_true.unique())}")
        print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è y_pred: {sorted(y_pred.unique())}")

        macro_f1 = f1_score(y_true, y_pred, average='macro')
        accuracy = accuracy_score(y_true, y_pred)
        report = classification_report(y_true, y_pred, output_dict=True)

        return {
            "macro_f1": macro_f1,
            "accuracy": accuracy,
            "detailed_report": report,
            "columns_used": {
                "predictions": sentiment_col,
                "ground_truth": truth_col
            },
            "data_info": {
                "samples_used": len(y_true),
                "true_labels_distribution": y_true.value_counts().to_dict(),
                "pred_labels_distribution": y_pred.value_counts().to_dict()
            }
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ evaluate: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))