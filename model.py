import pandas as pd
import torch
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import os
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
import numpy as np
from sklearn.metrics import f1_score
import joblib


class RuBertSentimentClassifier:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.classifier = None
        self.device = 0 if torch.cuda.is_available() else -1
        self.is_trained = False

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text) or not isinstance(text, str):
            return ""
        text = re.sub(r'https?://\S+|www\.\S+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\S+@\S+', '', text)
        return text.strip()

    def load_pretrained(self, model_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            print(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {model_path}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
            required_files = ['config.json', 'model.safetensors']
            for file in required_files:
                if not os.path.exists(os.path.join(model_path, file)):
                    print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: {file}")
                    return False

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            if os.path.exists(os.path.join(model_path, 'tokenizer_config.json')):
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π
                self.tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased-sentence")
                print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            self.classifier = pipeline(
                "text-classification",
                model=self.model,
                tokenizer=self.tokenizer,
                device=self.device,
                truncation=True,
                max_length=128,
                batch_size=32
            )

            self.is_trained = True
            print("‚úÖ RuBERT –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def train(self, texts, labels, model_name="DeepPavlov/rubert-base-cased-sentence"):
        """Fine-tuning –º–æ–¥–µ–ª–∏ RuBERT (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ)"""
        try:
            print("üîÑ –ó–∞–ø—É—Å–∫ fine-tuning RuBERT...")
            # ... –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è ...
            # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è
            pass
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è RuBERT: {e}")
            raise

    def predict(self, texts):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.is_trained or self.classifier is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏.")

        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        cleaned_texts = [self.clean_text(text) for text in texts]

        try:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            results = self.classifier(cleaned_texts)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ (LABEL_0 -> 0, LABEL_1 -> 1, LABEL_2 -> 2)
            predictions = [int(r["label"].replace("LABEL_", "")) for r in results]

            return predictions

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return [1] * len(texts)

    def save(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è –±—É–¥—É—â–µ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è)"""
        if self.is_trained and self.model is not None:
            model_path = os.path.join(path, "rubert_model")
            os.makedirs(model_path, exist_ok=True)
            self.model.save_pretrained(model_path)
            if self.tokenizer:
                self.tokenizer.save_pretrained(model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")

    def load(self, path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_path = os.path.join(path, "rubert_model")
        if os.path.exists(model_path):
            return self.load_pretrained(model_path)
        else:
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
            return False